{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b251a85d-8ce9-4947-997b-faa8437648e1",
   "metadata": {},
   "source": [
    "# MOET structure learning via variational inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c5cb56-3668-4652-8316-3d90477b4fe5",
   "metadata": {},
   "source": [
    "We start with an unnormalized measure:\n",
    "$$\n",
    "\\begin{align}\n",
    "&P(dz, x; w) :: M~\\mathbb{R} \\\\\n",
    "&P(dz, x; w) = \\textbf{do}\\{z \\leftarrow P_z;~\\textbf{return}~R(z; w, x)\\} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We define the density of the unnormalized measure (and assume $P_z$ has access to its own density):\n",
    "$$\n",
    "\\begin{align}\n",
    "P(z, x; w) = P_z(z) * R(z; w, x)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $P_z$ is a prior over _trees_ ($z$) and $R$ is a reward function which returns the _score of the data $x$_ using probabilistic circuit with tree structure $z$ and weights $w$.\n",
    "\n",
    "## Variational inference\n",
    "\n",
    "We seek to learn a proposal over tree structure using variational inference. We construct two gradient estimators: we start with the log marginal likelihood of the data objective, and arrive at the evidence lower bound (ELBO):\n",
    "\n",
    "$$L(w, \\theta; x) = \\log P(x; w) = \\log E_{z \\sim Q(dz; \\theta)}[\\frac{P(z, x; w)}{ Q(z; \\theta)}] \\geq \\underbrace{E_{z \\sim Q(dz; \\theta)}[\\log P(z, x; w) - \\log Q(z; \\theta)]}_{ELBO}$$\n",
    "\n",
    "Let $\\mathcal{L}(z, x; w, \\theta) = \\log P(z, x; w) - \\log Q(z; \\theta)$. From the ELBO, we derive two estimators:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_w L(w, \\theta; x) &= E_{z \\sim Q(dz; Î¸)}[\\nabla_w \\log R(z, x; w)] \\\\\n",
    "\\nabla_\\theta L(w, \\theta; x) &= E_{z \\sim Q(dz; \\theta)}[\\nabla_\\theta \\mathcal{L}(r; w, \\theta, x) + \\mathcal{L}(r; w, \\theta, x) \\times \\nabla_\\theta \\log Q(r; \\theta)]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### Improved gradient estimator efficiency via auxiliary variables\n",
    "\n",
    "The performance of the REINFORCE estimator depends greatly on the structure of $R(z; w, x)$. Because REINFORCE will only provide non-zero gradient signal when (TODO)\n",
    "\n",
    "In other words, if $Q(dz; \\theta)$ spreads its probability mass away from the support of the unnormalized $P$, the estimator loses efficiency. We can correct for this effect by constructing a proposal $Q$ which builds a valid (on support) tree $z$.\n",
    "\n",
    "We write:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&Q(dz; \\theta) :: M~\\mathcal{Z} \\\\\n",
    "&Q(dz; \\theta) = \\textbf{do}\\{ r \\leftarrow Q_r(\\theta); z \\leftarrow Q_z(r); \\textbf{return}~z \\}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The REINFORCE estimator requires that we evaluate the density of $Q$, but now the distribution over $z$ which $Q$ defines involves a projective pushforward of a joint distribution over $(r, z)$ (a pushforward which forgets $r$, which is equivalent to marginalizing $r$ out of the joint distribution). \n",
    "\n",
    "We can estimate the density of $Q(z; \\theta)$ by constructing an importance sampling estimate of the density. First, we define the joint:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&Q'(dr, dz; \\theta) :: D~(\\mathcal{R} \\times \\mathcal{Z}) \\\\\n",
    "&Q'(dr, dz; \\theta) = \\textbf{do}\\{ r \\leftarrow Q_r(\\theta); z \\leftarrow Q_z(r); \\textbf{return}~(r, z) \\}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $D$ denotes a measure with normalized density. Now, we construct a sampling estimator:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\chi(dz, dw; \\theta) :: M~(\\mathcal{Z} \\times \\mathcal{R}) \\\\\n",
    "&\\chi(dz, dw; \\theta) = \\textbf{do}\\{ (r, z) \\leftarrow Q'(\\theta); \\textbf{return} (z, \\frac{Q''(r; z)}{Q'(r, z)}) \\}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The term $\\frac{1}{w} = \\frac{Q''(r; z)}{Q'(r, z)}$ is the reciprocal of an importance weight $w$. Under the sampler $Q(dz; \\theta) = \\textbf{proj}_0~\\chi(dz, dw; \\theta)$, the property of the weight is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}_{z \\sim Q(dz; \\theta)}[\\frac{1}{w}] = \\frac{1}{Q(z; \\theta)}\n",
    "\\end{align}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
